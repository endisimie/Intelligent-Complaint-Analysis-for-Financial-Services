{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbaaae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\10 Acadamy\\week 6\\Intelligent-Complaint-Analysis-for-Financial-Services\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "import torch # For device management and dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9648a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "VECTOR_STORE_DIR = '../data/vector_store'\n",
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "# --- LOCAL LLM CONFIGURATION ---\n",
    "# Using Zephyr-7B-Beta for local inference\n",
    "# You might need to experiment with other quantized versions (e.g., GGUF via ctransformers or llama_cpp_python)\n",
    "# if you face memory issues or extreme slowness on your specific hardware.\n",
    "# 'HuggingFaceH4/zephyr-7b-beta' is the model ID\n",
    "# For local inference, direct model loading is usually better than HuggingFaceHub for custom params\n",
    "LLM_MODEL_LOCAL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# --- ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d887c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model(model_name):\n",
    "    \"\"\"\n",
    "    Loads the HuggingFace embedding model, ensuring GPU usage if available.\n",
    "    (Copied from vector_store_indexing.py for consistency, though it's typically loaded once)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Loading embedding model: {model_name} ---\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device for embeddings: {device}\")\n",
    "    try:\n",
    "        embeddings_model = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': device}\n",
    "        )\n",
    "        print(\"Embedding model loaded successfully.\")\n",
    "        return embeddings_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embedding model {model_name}: {e}\")\n",
    "        print(\"Please ensure you have 'sentence-transformers' and 'torch' installed.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe2369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(persist_directory, embedding_function):\n",
    "    \"\"\"\n",
    "    Loads the persisted ChromaDB vector store.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Loading vector store from {persist_directory} ---\")\n",
    "    if not os.path.exists(persist_directory):\n",
    "        print(f\"Error: Vector store directory '{persist_directory}' not found. Please run Task 2 first.\")\n",
    "        return None\n",
    "    try:\n",
    "        vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)\n",
    "        print(\"Vector store loaded successfully.\")\n",
    "        return vectordb\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading vector store: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b3d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_llm_model(model_name):\n",
    "    \"\"\"\n",
    "    Loads a local LLM using HuggingFace Transformers pipeline.\n",
    "    This will attempt to use GPU if available. Quantization is applied.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Loading local LLM model: {model_name} ---\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # Load the model with 4-bit quantization for efficiency\n",
    "        # requires `bitsandbytes` and `accelerate`\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16, # Use float16 for efficiency on GPU\n",
    "            device_map=\"auto\",         # Automatically places model parts on GPU/CPU\n",
    "            load_in_4bit=True          # Enable 4-bit quantization\n",
    "        )\n",
    "        # Ensure the pad_token is set for generation, especially important for batching or some pipelines\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(f\"Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "\n",
    "        # Create a text-generation pipeline\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=512,  # Max tokens for the LLM to generate in response\n",
    "            temperature=0.7,     # Controls randomness of output\n",
    "            top_p=0.95,          # Nucleus sampling\n",
    "            repetition_penalty=1.1 # Avoids repetitive text\n",
    "        )\n",
    "\n",
    "        llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        print(f\"Local LLM '{model_name}' loaded successfully. Device map: {model.hf_device_map}\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading local LLM model {model_name}: {e}\")\n",
    "        print(\"Please ensure you have 'transformers', 'torch', 'accelerate', and 'bitsandbytes' installed.\")\n",
    "        print(\"If you have an NVIDIA GPU, ensure CUDA is properly set up.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da5fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_rag_system(vector_store, llm):\n",
    "    \"\"\"\n",
    "    Implements the RAG system using a loaded LLM and a vector store.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Implementing RAG System ---\")\n",
    "\n",
    "    # Define the prompt template\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer as concise as possible.\n",
    "    Always say \"Thanks for asking!\" at the end of the answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    if vector_store is None:\n",
    "        print(\"Error: Vector store is not loaded. Cannot implement RAG system.\")\n",
    "        return None\n",
    "    if llm is None:\n",
    "        print(\"Error: LLM model is not loaded. Cannot implement RAG system.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Create the RetrievalQA chain\n",
    "        rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=vector_store.as_retriever(search_kwargs={\"k\": 5}), # Retrieve top 5 relevant chunks\n",
    "            return_source_documents=True, # Important for evaluation to see retrieved context\n",
    "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "        )\n",
    "        print(\"RAG system implemented successfully.\")\n",
    "        return rag_chain\n",
    "    except Exception as e:\n",
    "        print(f\"Error implementing RAG system: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a12e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualitative_evaluation(rag_system, questions):\n",
    "    \"\"\"\n",
    "    Performs a qualitative evaluation of the RAG system using representative questions.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Qualitative Evaluation ---\")\n",
    "    if rag_system is None:\n",
    "        print(\"RAG system not initialized. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    for i, item in enumerate(questions):\n",
    "        question = item[\"question\"]\n",
    "        product_filter = item.get(\"product_filter\") # Optional filter\n",
    "        print(f\"\\n--- Query {i+1}: ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        if product_filter:\n",
    "            print(f\"Product Filter: {product_filter}\")\n",
    "\n",
    "        # The `invoke` method is preferred for chains in newer LangChain versions\n",
    "        # It allows for more direct input and structured output access\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # If you need to add filters directly to the retriever, you'd modify how the retriever is created\n",
    "            # For simplicity here, we assume the retriever works on the full vector store.\n",
    "            # For product-specific filtering, you'd need to re-initialize the retriever with a query_kwargs\n",
    "            # or a custom retriever that incorporates metadata filters before the similarity search.\n",
    "            # Example (conceptual):\n",
    "            # retriever = vector_store.as_retriever(search_kwargs={\"k\": 5, \"filter\": {\"product\": product_filter}})\n",
    "\n",
    "            # For now, let's keep it simple without dynamic filtering unless your ChromaDB supports it out-of-the-box with .as_retriever()\n",
    "            # If you need robust metadata filtering with Chroma, you'd define the retriever like this:\n",
    "            # retriever = vector_store.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "            #                                       search_kwargs={\"score_threshold\": 0.7, \"k\": 5, \"filter\": {\"product\": product_filter}})\n",
    "            # However, `filter` typically expects `Eq`, `Gt`, `Lt`, etc. for comparison.\n",
    "            # For basic equality, a dict can work.\n",
    "\n",
    "            result = rag_system.invoke({\"query\": question})\n",
    "            response_time = time.time() - start_time\n",
    "\n",
    "            print(f\"\\nAnswer: {result['result']}\")\n",
    "            print(f\"Response Time: {response_time:.2f} seconds\")\n",
    "\n",
    "            print(\"\\n--- Retrieved Source Documents: ---\")\n",
    "            if result.get('source_documents'):\n",
    "                for j, doc in enumerate(result['source_documents']):\n",
    "                    print(f\"  Source {j+1}:\")\n",
    "                    print(f\"    Content (first 200 chars): {doc.page_content[:200]}...\")\n",
    "                    print(f\"    Metadata: {doc.metadata}\")\n",
    "            else:\n",
    "                print(\"  No source documents retrieved.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error during query '{question}': {e}\")\n",
    "            print(\"  Skipping this question.\")\n",
    "\n",
    "    print(\"\\n--- Qualitative Evaluation Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6e35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Task 3: Implement RAG System and Qualitative Evaluation\n",
      "\n",
      "--- Loading embedding model: sentence-transformers/all-MiniLM-L6-v2 ---\n",
      "Using device for embeddings: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_27856\\2673214196.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully.\n",
      "\n",
      "--- Loading vector store from ../data/vector_store ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_27856\\3454176586.py:10: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded successfully.\n",
      "\n",
      "--- Loading local LLM model: HuggingFaceH4/zephyr-7b-beta ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    REPORTS_DIR = 'reports/'\n",
    "    if not os.path.exists(REPORTS_DIR):\n",
    "        os.makedirs(REPORTS_DIR)\n",
    "        print(f\"Created directory: {REPORTS_DIR}\")\n",
    "\n",
    "    print(\"Starting Task 3: Implement RAG System and Qualitative Evaluation\")\n",
    "\n",
    "    # 1. Load Embedding Model (Needed for Vector Store)\n",
    "    embeddings = get_embedding_model(EMBEDDING_MODEL_NAME)\n",
    "    if embeddings is None:\n",
    "        print(\"Failed to load embedding model. Exiting Task 3.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Load Vector Store\n",
    "    vectordb = load_vector_store(VECTOR_STORE_DIR, embeddings)\n",
    "    if vectordb is None:\n",
    "        print(\"Failed to load vector store. Exiting Task 3.\")\n",
    "        exit()\n",
    "\n",
    "    # 3. Load Local LLM\n",
    "    llm_model = get_local_llm_model(LLM_MODEL_LOCAL_NAME)\n",
    "    if llm_model is None:\n",
    "        print(\"Failed to load local LLM model. Exiting Task 3.\")\n",
    "        exit()\n",
    "\n",
    "    # 4. Implement RAG System\n",
    "    rag_system = implement_rag_system(vectordb, llm_model)\n",
    "    if rag_system is None:\n",
    "        print(\"Failed to implement RAG system. Exiting Task 3.\")\n",
    "        exit()\n",
    "\n",
    "    # 5. Define Representative Questions for Qualitative Evaluation\n",
    "    representative_questions = [\n",
    "        {\"question\": \"What are the common issues people face with their credit cards?\", \"product_filter\": \"Credit card\"},\n",
    "        {\"question\": \"Why are customers unhappy with Buy Now, Pay Later services?\", \"product_filter\": \"Buy Now, Pay Later\"},\n",
    "        {\"question\": \"Summarize complaints about bank accounts.\", \"product_filter\": \"Bank account or service\"},\n",
    "        {\"question\": \"What problems do consumers encounter with mortgages?\", \"product_filter\": \"Mortgage\"},\n",
    "        {\"question\": \"Tell me about complaints regarding loan application rejections.\", \"product_filter\": \"Personal loan\"}\n",
    "    ]\n",
    "\n",
    "    # Perform qualitative evaluation\n",
    "    qualitative_evaluation(rag_system, representative_questions)\n",
    "\n",
    "    print(\"\\n--- Task 3 Completed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
