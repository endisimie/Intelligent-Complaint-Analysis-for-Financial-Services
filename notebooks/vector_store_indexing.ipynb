{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fa8c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\10 Acadamy\\week 6\\Intelligent-Complaint-Analysis-for-Financial-Services\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import tqdm # For progress bar\n",
    "\n",
    "# Configuration\n",
    "DATA_PROCESSED_PATH = '../data/processed/filtered_complaints.csv'\n",
    "VECTOR_STORE_DIR = '../data/vector_store'\n",
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1500  # characters\n",
    "CHUNK_OVERLAP = 200 # characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b5c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the cleaned and filtered complaint dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded processed data from {file_path}. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Processed data file not found at {file_path}. Please run Task 1 first.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading processed data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d711df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_texts(df, text_column='Consumer complaint narrative_cleaned', id_column='Complaint ID', product_column='Product'):\n",
    "    \"\"\"\n",
    "    Chunks long text narratives and associates metadata.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Chunking text narratives (chunk_size={CHUNK_SIZE}, chunk_overlap={CHUNK_OVERLAP}) ---\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,  # Measure length by characters\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Default separators\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    # Create a unique ID for each original complaint, assuming 'Complaint ID' exists or generating one.\n",
    "    # If 'Complaint ID' is not in your CFPB data, you might need to create one, e.g., using df.index\n",
    "    if id_column not in df.columns:\n",
    "        df[id_column] = range(len(df)) # Simple integer ID if not present\n",
    "\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"Chunking complaints\"):\n",
    "        narrative = row[text_column]\n",
    "        original_complaint_id = row[id_column]\n",
    "        product_category = row[product_column]\n",
    "        issue = row['Issue'] # Assuming 'Issue' is a useful metadata field\n",
    "\n",
    "        # Ensure narrative is a string, handle potential NaN/empty after cleaning\n",
    "        if pd.isna(narrative) or not isinstance(narrative, str) or len(narrative.strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        chunks = text_splitter.split_text(narrative)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                \"chunk_content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"original_complaint_id\": original_complaint_id,\n",
    "                    \"product\": product_category,\n",
    "                    \"issue\": issue,\n",
    "                    \"chunk_id\": f\"{original_complaint_id}-{i}\" # Unique ID for each chunk\n",
    "                }\n",
    "            })\n",
    "    print(f\"Generated {len(all_chunks)} chunks.\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6534f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model(model_name):\n",
    "    \"\"\"\n",
    "    Loads the HuggingFace embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Loading embedding model: {model_name} ---\")\n",
    "    try:\n",
    "        # Use HuggingFaceEmbeddings from LangChain for consistency with Chroma\n",
    "        embeddings_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        print(\"Embedding model loaded successfully.\")\n",
    "        return embeddings_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embedding model {model_name}: {e}\")\n",
    "        print(\"Please ensure you have 'sentence-transformers' and 'torch' installed.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a2d7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ../data/vector_store\n",
      "Starting Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
      "Loaded processed data from ../data/processed/filtered_complaints.csv. Shape: (1581308, 5)\n",
      "\n",
      "--- Chunking text narratives (chunk_size=1500, chunk_overlap=200) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking complaints: 100%|██████████| 1581308/1581308 [06:15<00:00, 4208.92it/s]\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16560\\837609308.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1959511 chunks.\n",
      "\n",
      "--- Loading embedding model: sentence-transformers/all-MiniLM-L6-v2 ---\n",
      "Embedding model loaded successfully.\n",
      "\n",
      "--- Creating and persisting vector store to ../data/vector_store ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16560\\1376949233.py:25: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted successfully.\n",
      "\n",
      "Vector store containing 1959511 chunks is ready for querying.\n",
      "\n",
      "--- Verifying Vector Store (Optional) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16560\\1376949233.py:70: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  loaded_vectordb = Chroma(persist_directory=VECTOR_STORE_DIR, embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample query result:\n",
      "Content: find this to be an unfair and unreasonable policy, particularly when other major credit card issuers offer one-time exceptions to responsible, long-term customers who experience unforeseen financial d...\n",
      "Metadata: {'original_complaint_id': 228264, 'chunk_id': '228264-1', 'issue': 'Fees or interest', 'product': 'Credit card'}\n",
      "\n",
      "--- Task 2 Completed ---\n"
     ]
    }
   ],
   "source": [
    "def create_and_persist_vector_store(chunks, embeddings_model, persist_directory):\n",
    "    \"\"\"\n",
    "    Creates a ChromaDB vector store and persists it to disk.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Creating and persisting vector store to {persist_directory} ---\")\n",
    "\n",
    "    # Extract texts and metadatas from the chunks list\n",
    "    texts = [chunk[\"chunk_content\"] for chunk in chunks]\n",
    "    metadatas = [chunk[\"metadata\"] for chunk in chunks]\n",
    "\n",
    "    if not texts:\n",
    "        print(\"No chunks to embed. Vector store will not be created.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Create Chroma vector store from documents (texts + metadatas)\n",
    "        # LangChain's Chroma.from_documents expects `Document` objects or lists of texts and metadatas\n",
    "        # For simplicity, we directly use texts and metadatas\n",
    "        vectordb = Chroma.from_texts(\n",
    "            texts=texts,\n",
    "            embedding=embeddings_model,\n",
    "            metadatas=metadatas,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        vectordb.persist()\n",
    "        print(\"Vector store created and persisted successfully.\")\n",
    "        return vectordb\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or persisting vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure vector store directory exists\n",
    "    if not os.path.exists(VECTOR_STORE_DIR):\n",
    "        os.makedirs(VECTOR_STORE_DIR)\n",
    "        print(f\"Created directory: {VECTOR_STORE_DIR}\")\n",
    "\n",
    "    print(\"Starting Task 2: Text Chunking, Embedding, and Vector Store Indexing\")\n",
    "\n",
    "    # 1. Load processed data\n",
    "    df_cleaned = load_processed_data(DATA_PROCESSED_PATH)\n",
    "\n",
    "    if df_cleaned is not None:\n",
    "        # 2. Chunk text narratives\n",
    "        # We'll use 'Complaint ID' as the ID column. If your CSV doesn't have it,\n",
    "        # the load_processed_data function should have added it in Task 1's script\n",
    "        # based on `df.index` or a similar mechanism.\n",
    "        # Ensure 'Product' and 'Issue' columns are present in df_cleaned.\n",
    "        required_cols = ['Consumer complaint narrative_cleaned', 'Product', 'Issue']\n",
    "        for col in required_cols:\n",
    "            if col not in df_cleaned.columns:\n",
    "                print(f\"Error: Required column '{col}' not found in the processed data. Please check Task 1 output.\")\n",
    "                exit() # Exit if crucial columns are missing\n",
    "\n",
    "        chunks = chunk_texts(df_cleaned, id_column='Complaint ID') # Assuming 'Complaint ID' is now available\n",
    "\n",
    "        if chunks: # Only proceed if chunks were generated\n",
    "            # 3. Choose and load an embedding model\n",
    "            embeddings = get_embedding_model(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "            if embeddings is not None:\n",
    "                # 4. Embed and Index\n",
    "                vectordb = create_and_persist_vector_store(chunks, embeddings, VECTOR_STORE_DIR)\n",
    "\n",
    "                if vectordb:\n",
    "                    print(f\"\\nVector store containing {len(chunks)} chunks is ready for querying.\")\n",
    "                    # Optional: Verify by loading and querying a small sample\n",
    "                    print(\"\\n--- Verifying Vector Store (Optional) ---\")\n",
    "                    try:\n",
    "                        loaded_vectordb = Chroma(persist_directory=VECTOR_STORE_DIR, embedding_function=embeddings)\n",
    "                        query_results = loaded_vectordb.similarity_search(\"Why are people unhappy with their credit card?\", k=1)\n",
    "                        if query_results:\n",
    "                            print(\"\\nSample query result:\")\n",
    "                            print(f\"Content: {query_results[0].page_content[:200]}...\")\n",
    "                            print(f\"Metadata: {query_results[0].metadata}\")\n",
    "                        else:\n",
    "                            print(\"No results for sample query. Vector store might be empty or query too specific.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during vector store verification: {e}\")\n",
    "                else:\n",
    "                    print(\"Failed to create and persist vector store.\")\n",
    "            else:\n",
    "                print(\"Failed to load embedding model. Exiting Task 2.\")\n",
    "        else:\n",
    "            print(\"No valid chunks generated. Exiting Task 2.\")\n",
    "    else:\n",
    "        print(\"Processed data loading failed. Exiting Task 2.\")\n",
    "\n",
    "    print(\"\\n--- Task 2 Completed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb929b",
   "metadata": {},
   "source": [
    "Explanation of Choices for the Report:\n",
    "\n",
    "Chunking Strategy (RecursiveCharacterTextSplitter):\n",
    "\n",
    "Why RecursiveCharacterTextSplitter? This splitter is generally recommended for its robustness. It attempts to split text using a list of characters ([\"\\n\\n\", \"\\n\", \" \", \"\"] by default) in order, trying to keep semantically related units (like paragraphs, sentences, words) together. This is crucial for complaint narratives, where retaining the context of an issue is vital for effective retrieval.\n",
    "\n",
    "chunk_size and chunk_overlap:\n",
    "\n",
    "chunk_size: This parameter determines the maximum number of characters (or tokens, depending on the length_function) in each chunk. For complaint data, which can vary greatly in length, a chunk_size that allows for a complete thought or specific complaint detail is ideal. Too small, and context is lost; too large, and the embedding might become too generic, or exceed the embedding model's input token limit. A common range for RAG applications is 128-512 tokens. Given that all-MiniLM-L6-v2 handles up to 256 word pieces, we'll aim for character counts that translate to a similar token count. A chunk_size of 500 characters is a good starting point, as it's typically enough to capture a significant part of a complaint without being excessively long. This usually translates to around 100-150 tokens, well within the model's capacity.\n",
    "\n",
    "chunk_overlap: This specifies the number of characters that will overlap between consecutive chunks. Overlap is crucial to prevent the loss of context at the boundaries of chunks. If a key piece of information spans two chunks, overlap ensures that both chunks contain some shared context, improving the chances of retrieval. An overlap of 10-20% of the chunk_size is generally effective. For a chunk_size of 500, a chunk_overlap of 50-100 characters is reasonable. We'll use 100 to ensure good context flow.\n",
    "\n",
    "Justification: The chosen chunk_size aims to capture sufficient context from a complaint narrative while staying within the limits of the embedding model. The chunk_overlap ensures that semantic continuity is maintained across chunk boundaries, which is critical for questions that might relate to information spanning multiple segments of a complaint. These values are a good balance between retaining context and preventing overly large chunks that dilute the semantic meaning or exceed model limits.\n",
    "\n",
    "Embedding Model (sentence-transformers/all-MiniLM-L6-v2):\n",
    "\n",
    "Why all-MiniLM-L6-v2?\n",
    "\n",
    "Efficiency: It's a compact and efficient model (384-dimensional vectors) that provides a good balance between performance and computational cost. This is important for processing a large volume of complaints and for efficient real-time querying in a production environment.\n",
    "\n",
    "Performance for Semantic Search: This model is specifically trained for sentence and short paragraph embeddings, making it highly effective for semantic similarity tasks, which is precisely what our RAG system needs for retrieving relevant complaint narratives. It captures the semantic meaning well, even if the exact keywords aren't present in the query.\n",
    "\n",
    "Open-source and readily available: It's a widely used and well-supported model from the sentence-transformers library, making it easy to integrate and leverage.\n",
    "\n",
    "Vector Store (ChromaDB):\n",
    "\n",
    "Why ChromaDB?\n",
    "\n",
    "Ease of Use and Local Persistence: ChromaDB is a lightweight, easy-to-use vector database that supports local persistence. This makes it straightforward to set up and manage for an internal tool like CrediTrust's, without requiring complex infrastructure.\n",
    "\n",
    "Metadata Support: ChromaDB allows storing rich metadata alongside vectors, which is essential for our requirement to link retrieved chunks back to their original complaint ID and product category. This will enable filtering and more insightful answers.\n",
    "\n",
    "Integration with LangChain: ChromaDB has excellent integration with LangChain, simplifying the process of creating and querying the vector store.\n",
    "\n",
    "Scalability for Initial Needs: While not as distributed as some larger vector databases, ChromaDB is perfectly suitable for a user base of 500,000 and thousands of complaints per month, especially in its initial internal tool phase. It can scale to a decent volume of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
